# train.py ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π

# === –ò–º–ø–æ—Ä—Ç –±–∞–∑–æ–≤—ã—Ö –º–æ–¥—É–ª–µ–π ===
import os                   # –†–∞–±–æ—Ç–∞ —Å —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–æ–π
import logging              # –î–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ –∫–æ–Ω—Å–æ–ª—å –∏–ª–∏ —Ñ–∞–π–ª)
from datetime import datetime  # –î–ª—è –∑–∞–ø–∏—Å–∏ —Ç–µ–∫—É—â–µ–π –¥–∞—Ç—ã –∏ –≤—Ä–µ–º–µ–Ω–∏

# === –ò–º–ø–æ—Ä—Ç PyTorch –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è ===
import torch
import torch.nn as nn                       # –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ —Å–ª–æ–∏ –∏ —Ñ—É–Ω–∫—Ü–∏–∏
import torch.optim as optim                 # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä AdamW
from torch.utils.tensorboard import SummaryWriter  # –î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ TensorBoard
from tqdm import tqdm                       # –£–¥–æ–±–Ω–∞—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –±–∏–±–ª–∏–æ—Ç–µ–∫–∞
from torch.amp import autocast, GradScaler  # –î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ AMP (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Å–º–µ—à–∞–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å)
from sklearn.metrics import f1_score        # –ú–µ—Ç—Ä–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

# === –ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π –æ—Ç PyTorch ===
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="torch.optim.lr_scheduler")

# === –ò–º–ø–æ—Ä—Ç –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥—É–ª–µ–π –ø—Ä–æ–µ–∫—Ç–∞ ===
from dataloader import get_dataloaders       # –§—É–Ω–∫—Ü–∏—è, –≤–æ–∑–≤—Ä–∞—â–∞—é—â–∞—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö
from model import get_model                  # –§—É–Ω–∫—Ü–∏—è, —Å–æ–∑–¥–∞—é—â–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—É—é –º–æ–¥–µ–ª—å

# === –†–µ–∞–ª–∏–∑–∞—Ü–∏—è FocalLoss ‚Äî —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è CrossEntropy –¥–ª—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ ===
class FocalLoss(nn.Module):
    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha       # –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ (–¥–ª—è –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞)
        self.gamma = gamma       # –ü–∞—Ä–∞–º–µ—Ç—Ä —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏ (–±–æ–ª—å—à–µ –∑–Ω–∞—á–µ–Ω–∏–µ -> —Å–∏–ª—å–Ω–µ–µ —Ñ–æ–∫—É—Å –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö)
        self.reduction = reduction  # –ú–µ—Ç–æ–¥ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è: 'mean', 'sum', –ª–∏–±–æ –Ω–∏—á–µ–≥–æ

    def forward(self, logits, targets):
        ce_loss = nn.functional.cross_entropy(logits, targets, reduction='none', weight=self.alpha)
        pt = torch.exp(-ce_loss)  # –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
        focal_loss = (1 - pt) ** self.gamma * ce_loss

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

# === –§—É–Ω–∫—Ü–∏—è –∑–∞–º–æ—Ä–æ–∑–∫–∏ BatchNorm —Å–ª–æ—ë–≤ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏ transfer learning) ===
def freeze_batchnorm(model):
    for module in model.modules():
        if isinstance(module, nn.BatchNorm2d) or isinstance(module, nn.BatchNorm1d):
            for param in module.parameters():
                param.requires_grad = False  # –û—Ç–∫–ª—é—á–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ BatchNorm

# === –§—É–Ω–∫—Ü–∏—è —Ä–∞–∑–º–æ—Ä–æ–∑–∫–∏ BatchNorm —Å–ª–æ—ë–≤ ===
def unfreeze_batchnorm(model):
    for module in model.modules():
        if isinstance(module, nn.BatchNorm2d) or isinstance(module, nn.BatchNorm1d):
            module.train()  # –í–∫–ª—é—á–∞–µ–º —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
            for param in module.parameters():
                param.requires_grad = True

# === –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Å–ª–æ—ë–≤ –ø–æ —É—Ä–æ–≤–Ω—è–º (–¥–ª—è LLRD ‚Äî layer-wise learning rate decay) ===
def get_layer_groups_for_llrd(features, model_name):
    if model_name == "efficientnet_b0":
        if isinstance(features, nn.Sequential):
            return list(features[0].children())
        else:
            return list(features.features.children())  # –¥–ª—è EfficientNet –±–µ–∑ ArcFace
    elif model_name == "swin_tiny":
        return list(features.model.features.children())  # —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ Swin-T
    else:
        return list(features.children())  # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Å–ª—É—á–∞–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, ResNet)


# === –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ ===
def train_model(model_name):
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—É—Ç–µ–π –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è
    dataset_dir = "dataset_10classes"                # –ü–∞–ø–∫–∞ —Å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º
    model_dir = "models"                             # –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π (—á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤)
    batch_size = 32                                   # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (—Å–∫–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø–æ–¥–∞—ë—Ç—Å—è –≤ –º–æ–¥–µ–ª—å)

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ
    if model_name == "resnet50":
        epochs = 80                                    # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
        patience = 15                                  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è, –ø–æ—Å–ª–µ –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç early stopping
        warmup_epochs = 5                              # –°–∫–æ–ª—å–∫–æ —ç–ø–æ—Ö –º–æ–¥–µ–ª—å "—Ä–∞–∑–æ–≥—Ä–µ–≤–∞–µ—Ç—Å—è" –ø–µ—Ä–µ–¥ –≤–∫–ª—é—á–µ–Ω–∏–µ–º ArcFace
        base_lr = 1.5e-4                               # –ë–∞–∑–æ–≤–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        llrd_decay = 0.8                               # –§–∞–∫—Ç–æ—Ä –∑–∞—Ç—É—Ö–∞–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Å–ª–æ—è–º
        freeze_schedule = [5]                          # –ù–∞ –∫–∞–∫–æ–π —ç–ø–æ—Ö–µ —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞—Ç—å —Å–ª–æ–∏
    elif model_name == "efficientnet_b0":
        epochs = 90
        patience = 20
        warmup_epochs = 5
        base_lr = 2e-4
        llrd_decay = 0.9
        freeze_schedule = [5]
    elif model_name == "swin_tiny":
        epochs = 150
        patience = 35
        warmup_epochs = 10
        base_lr = 2e-4
        llrd_decay = 0.95
        freeze_schedule = []  # swin –æ–±—É—á–∞–µ—Ç—Å—è —Å—Ä–∞–∑—É –ø–æ–ª–Ω–æ—Å—Ç—å—é

    # –ü—Ä–æ—á–∏–µ –æ–±—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    weight_decay = 1e-4                                # L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
    num_workers = 2                                    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö
    use_amp = True                                     # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Å–º–µ—à–∞–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å
    use_focal = True                                   # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Focal Loss (–∏–Ω–∞—á–µ CrossEntropyLoss)

    os.makedirs(model_dir, exist_ok=True)              # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    now = datetime.now().strftime("%Y%m%d_%H%M%S")     # –í—Ä–µ–º—è –∑–∞–ø—É—Å–∫–∞ (–¥–ª—è –ª–æ–≥–æ–≤ TensorBoard)
    writer = SummaryWriter(log_dir=os.path.join("runs", f"{now}_{model_name}_arcface"))

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # –ò—Å–ø–æ–ª—å–∑—É–µ–º GPU –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏
    logging.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ: {model_name} on {device}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ (–¥–ª—è –≤–∑–≤–µ—à–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å)
    train_loader, val_loader, _, class_weights = get_dataloaders(
        dataset_dir=dataset_dir,
        batch_size=batch_size,
        num_workers=num_workers,
        model_name=model_name,
        epoch=0  # –¥–ª—è Swin –≤—ã–±–∏—Ä–∞—é—Ç—Å—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ —ç–ø–æ—Ö–∞–º
    )
    logging.info("Class weights: [%s]", ", ".join(f"{w:.3f}" for w in class_weights.tolist()))

    freeze_base = model_name != "swin_tiny"           # —Ç–æ–ª—å–∫–æ swin –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –∑–∞–º–æ—Ä–æ–∑–∫–∏ –Ω–∞ —Å—Ç–∞—Ä—Ç–µ
    dropout_value = 0.2 if model_name == "swin_tiny" else 0.5  # –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ dropout

    # –ü–æ–ª—É—á–∞–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏ (features ‚Äî —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä, arc_head ‚Äî ArcFace –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä, feat_dim ‚Äî —Ä–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–∞)
    if model_name == "swin_tiny":
        features, arc_head, feat_dim = get_model(model_name, freeze_base=False, arcface=True, dropout_p=dropout_value)
        logging.info("‚úÖ Swin Transformer —Å—Ä–∞–∑—É –æ–±—É—á–∞–µ—Ç—Å—è —Å ArcFace.")
    else:
        features, arc_head, feat_dim = get_model(model_name, freeze_base=freeze_base, arcface=False, dropout_p=dropout_value)
        logging.info("üß™ Warm-up –Ω–∞—á–∞—Ç: ArcFace –æ—Ç–∫–ª—é—á—ë–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –≥–æ–ª–æ–≤–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.")

    # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –º–æ–¥–µ–ª—å –∏ –≥–æ–ª–æ–≤—É –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    features.to(device)
    if arc_head is not None:
        arc_head.to(device)

    # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º BatchNorm –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if freeze_base:
        freeze_batchnorm(features)
    else:
        unfreeze_batchnorm(features)

    # –í—ã–±–æ—Ä —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å: Focal –∏–ª–∏ CrossEntropy
    criterion = FocalLoss(alpha=class_weights.to(device), gamma=1.5) if use_focal else nn.CrossEntropyLoss(weight=class_weights.to(device))
    logging.info("üéØ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è FocalLoss —Å gamma=1.5" if use_focal else "üìò –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è CrossEntropyLoss")

    # === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ —Å LLRD ===
    # LLRD (Layer-wise Learning Rate Decay) ‚Äî –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –Ω–∏–∂–Ω–∏—Ö —Å–ª–æ—ë–≤
    layer_groups = get_layer_groups_for_llrd(features, model_name)  # —Ä–∞–∑–±–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –±–ª–æ–∫–∏
    param_groups = []  # —Å–ø–∏—Å–æ–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é –æ–±—É—á–µ–Ω–∏—è
    for i, layer in enumerate(layer_groups):
        decay_factor = llrd_decay ** (len(layer_groups) - i - 1)  # –±–æ–ª—å—à–µ –≥–ª—É–±–∏–Ω–∞ ‚Äî –º–µ–Ω—å—à–µ lr
        lr = base_lr * decay_factor
        param_groups.append({"params": layer.parameters(), "lr": lr})

    if arc_head is not None:
        param_groups.append(
            {"params": arc_head.parameters(), "lr": base_lr})  # –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –≤—Å–µ–≥–¥–∞ –æ–±—É—á–∞–µ—Ç—Å—è —Å base_lr

    optimizer = optim.AdamW(param_groups, weight_decay=weight_decay)  # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä AdamW

    # === –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è —Å warm-up –∏ cosine annealing ===
    from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR
    warmup_scheduler = LinearLR(optimizer, start_factor=0.1, total_iters=warmup_epochs)  # —Å–Ω–∞—á–∞–ª–∞ lr –ø–ª–∞–≤–Ω–æ —Ä–∞—Å—Ç—ë—Ç
    cosine_scheduler = CosineAnnealingLR(optimizer, T_max=epochs - warmup_epochs)  # –ø–æ—Ç–æ–º –∑–∞—Ç—É—Ö–∞–µ—Ç –ø–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –∫—Ä–∏–≤–æ–π
    scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler],
                             milestones=[warmup_epochs], last_epoch=-1)  # –∫–æ–º–±–∏–Ω–∏—Ä—É–µ–º

    skip_scheduler_step = False  # –µ—Å–ª–∏ True, —Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–¥–∏–Ω —à–∞–≥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞)
    scaler = GradScaler(enabled=use_amp)  # —Å–∫–µ–π–ª–µ—Ä –¥–ª—è AMP
    best_val_f1 = 0.0  # –õ—É—á—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ F1 –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    best_val_loss = float('inf')  # –õ—É—á—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ loss –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    epochs_no_improve = 0  # –°—á—ë—Ç—á–∏–∫ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏–π

    # === –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø–æ —ç–ø–æ—Ö–∞–º ===
    for epoch in range(1, epochs + 1):
        skipped_steps = 0  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –ø—Ä–æ–ø—É—â–µ–Ω—ã –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        logging.info(f"‚ñ∂Ô∏è Epoch {epoch}/{epochs} ({round(100 * epoch / epochs)}%) ‚Äî {model_name}")

        # === –ü–µ—Ä–µ—Ö–æ–¥ –æ—Ç warm-up –∫ –ø–æ–ª–Ω–æ–π —Ä–∞–∑–º–æ—Ä–æ–∑–∫–µ –∏ ArcFace ===
        if freeze_base and epoch in freeze_schedule:
            for param in features.parameters():
                param.requires_grad = True  # –†–∞–∑–º–æ—Ä–æ–∑–∫–∞ —Å–ª–æ—ë–≤
            unfreeze_batchnorm(features)  # –¢–∞–∫–∂–µ –∞–∫—Ç–∏–≤–∏—Ä—É–µ–º BatchNorm
            freeze_base = False
            logging.info(f"üîì –ü–æ–ª–Ω–∞—è —Ä–∞–∑–º–æ—Ä–æ–∑–∫–∞ {model_name} –Ω–∞ —ç–ø–æ—Ö–µ {epoch}")
            logging.info("üîÅ –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ warm-up: –≤–∫–ª—é—á–∞–µ–º ArcFace –∏ –ø–æ–ª–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—É—é –≥–æ–ª–æ–≤—É.")

            # –ü–æ–≤—Ç–æ—Ä–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —É–∂–µ —Å ArcFace-–≥–æ–ª–æ–≤–æ–π
            features, arc_head, feat_dim = get_model(model_name, freeze_base=False, arcface=True)
            features.to(device)
            arc_head.to(device)

            # –ü–æ–≤—Ç–æ—Ä–Ω–æ —Å–æ–∑–¥–∞—ë–º optimizer –∏ scheduler, —Ç–∞–∫ –∫–∞–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑–º–µ–Ω–∏–ª–∏—Å—å
            layer_groups = get_layer_groups_for_llrd(features, model_name)
            param_groups = []
            for i, layer in enumerate(layer_groups):
                decay_factor = llrd_decay ** (len(layer_groups) - i - 1)
                lr = base_lr * decay_factor
                param_groups.append({"params": layer.parameters(), "lr": lr})
            if arc_head is not None:
                param_groups.append({"params": arc_head.parameters(), "lr": base_lr})

            optimizer = optim.AdamW(param_groups, weight_decay=weight_decay)
            warmup_scheduler = LinearLR(optimizer, start_factor=0.1, total_iters=warmup_epochs)
            cosine_scheduler = CosineAnnealingLR(optimizer, T_max=epochs - warmup_epochs)
            scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler],
                                     milestones=[warmup_epochs], last_epoch=epoch - 1)
            skip_scheduler_step = True  # –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ lr –Ω–∞ —ç—Ç–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
            scaler = GradScaler(enabled=use_amp)
            logging.info("‚úÖ ArcFace —É—Å–ø–µ—à–Ω–æ –≤–∫–ª—é—á—ë–Ω –∏ optimizer/scheduler –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω—ã.")

        # === –†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ ===
        features.train()
        if arc_head is not None:
            arc_head.train()

        running_loss = 0  # –°—É–º–º–∞—Ä–Ω—ã–π loss –∑–∞ —ç–ø–æ—Ö—É (–±—É–¥–µ—Ç —É—Å—Ä–µ–¥–Ω—ë–Ω)
        correct = 0  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
        total = 0  # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤
        all_preds = []  # –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
        all_labels = []  # –°–ø–∏—Å–æ–∫ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫

        epoch_bar = tqdm(train_loader, desc="üî†", dynamic_ncols=True)  # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        for imgs, lbls in epoch_bar:
            imgs, lbls = imgs.to(device), lbls.to(device)  # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ GPU
            optimizer.zero_grad()  # –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã

            # === –í–∫–ª—é—á–∞–µ–º AMP (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Å–º–µ—à–∞–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å) ===
            with autocast(device_type='cuda', enabled=use_amp):
                feats = features(imgs)  # –ü–æ–ª—É—á–∞–µ–º –≤—ã—Ö–æ–¥ feature extractor'–∞
                if feats.ndim > 2:
                    feats = feats.view(feats.size(0), -1)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–Ω–∑–æ—Ä –≤ [batch, features] –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                if arc_head is not None:
                    outputs = arc_head(feats, lbls)  # ArcFace –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
                else:
                    outputs = feats  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –≥–æ–ª–æ–≤–∞ (–ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –≤ features)

                loss = criterion(outputs, lbls)  # –í—ã—á–∏—Å–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å

            scaler.scale(loss).backward()  # backward() —Å —É—á–µ—Ç–æ–º AMP

            if use_amp:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å—Ç—å –ª–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–≥–ª–∏ –±—ã—Ç—å –∑–∞–º–æ—Ä–æ–∂–µ–Ω—ã)
                has_grad = any(
                    p.grad is not None
                    for group in optimizer.param_groups
                    for p in group['params']
                    if p.requires_grad
                )
                if has_grad:
                    scaler.step(optimizer)  # —à–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
                    scaler.update()
                else:
                    skipped_steps += 1  # –µ—Å–ª–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –Ω–µ –±—ã–ª–æ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —à–∞–≥
            else:
                optimizer.step()  # –æ–±—ã—á–Ω—ã–π —à–∞–≥ (–±–µ–∑ AMP)

            # === –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ lr ===
            if not skip_scheduler_step:
                scheduler.step()
            else:
                skip_scheduler_step = False  # —Å–±—Ä–∞—Å—ã–≤–∞–µ–º —Ñ–ª–∞–≥, —á—Ç–æ–±—ã scheduler –ø—Ä–æ–¥–æ–ª–∂–∏–ª —Ä–∞–±–æ—Ç—É –Ω–∞ —Å–ª–µ–¥—É—é—â–∏—Ö –∏—Ç–µ—Ä–∞—Ü–∏—è—Ö

            # === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ ===
            running_loss += loss.item() * imgs.size(0)  # –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ ‚Äî –ø–æ–ª—É—á–∞–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π loss
            with torch.no_grad():
                preds = outputs.detach().argmax(dim=1)  # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã (–∏–Ω–¥–µ–∫—Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ç–∞)
                correct += (preds == lbls).sum().item()  # –ü–æ–¥—Å—á—ë—Ç –≤–µ—Ä–Ω—ã—Ö
                total += lbls.size(0)  # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                all_preds.extend(preds.cpu().tolist())
                all_labels.extend(lbls.cpu().tolist())
# === –ü–æ–¥—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫ –∑–∞ —ç–ø–æ—Ö—É ===
        epoch_loss = running_loss / total                        # –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –≤—Å–µ–º –æ–±—É—á–∞—é—â–∏–º –ø—Ä–∏–º–µ—Ä–∞–º
        epoch_acc = correct / total                              # –î–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ
        epoch_f1 = f1_score(all_labels, all_preds, average='macro')  # –£—Å—Ä–µ–¥–Ω—ë–Ω–Ω—ã–π –ø–æ –∫–ª–∞—Å—Å–∞–º F1-—Å–∫–æ—Ä (–º–∞–∫—Ä–æ-F1)

        # –í—ã–≤–æ–¥ –≤ –ª–æ–≥ —Å–≤–æ–¥–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ–±—É—á–µ–Ω–∏–∏
        logging.info(f"Train Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, F1: {epoch_f1:.4f}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è TensorBoard
        writer.add_scalar("Loss/train", epoch_loss, epoch)  # –ì—Ä–∞—Ñ–∏–∫ loss –ø–æ —ç–ø–æ—Ö–∞–º
        writer.add_scalar("Acc/train", epoch_acc, epoch)    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ —ç–ø–æ—Ö–∞–º
        writer.add_scalar("F1/train", epoch_f1, epoch)      # –ì—Ä–∞—Ñ–∏–∫ F1 –ø–æ —ç–ø–æ—Ö–∞–º

        # === –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ ===
        features.eval()                          # –ü–µ—Ä–µ–≤–æ–¥–∏–º feature extractor –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏ (–æ–±–Ω—É–ª—è–µ—Ç dropout, —Ñ–∏–∫—Å–∏—Ä—É–µ—Ç batchnorm)
        if arc_head is not None:
            arc_head.eval()                      # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏–º ArcFace-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä

        val_loss = 0                             # –°—É–º–º–∞—Ä–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        correct = 0                              # –°—á—ë—Ç—á–∏–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
        total = 0                                # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤
        val_preds = []                           # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤
        val_labels = []                          # –ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤

        # –û—Ç–∫–ª—é—á–∞–µ–º –ø–æ–¥—Å—á—ë—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏ —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        with torch.no_grad():
            for imgs, lbls in val_loader:
                imgs, lbls = imgs.to(device), lbls.to(device)  # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –º–µ—Ç–∫–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                feats = features(imgs)                          # –ü—Ä–æ–≥–æ–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ feature extractor
                if feats.ndim > 2:                              # –ï—Å–ª–∏ –≤—ã—Ö–æ–¥ –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, B√óC√óH√óW), —Ç–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º
                    feats = feats.view(feats.size(0), -1)

                if arc_head is not None:
                    outputs = arc_head(feats, lbls)            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ ArcFace (–µ—Å–ª–∏ —É–∂–µ –≤–∫–ª—é—á–µ–Ω–∞)
                else:
                    outputs = feats                            # –ò–Ω–∞—á–µ –±–µ—Ä—ë–º –ø—Ä—è–º–æ–π –≤—ã—Ö–æ–¥ feature extractor‚Äô–∞

                loss = criterion(outputs, lbls)                # –í—ã—á–∏—Å–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å
                val_loss += loss.item() * imgs.size(0)         # –£—á–∏—Ç—ã–≤–∞–µ–º loss —Å –≤–µ—Å–æ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É –±–∞—Ç—á–∞

                preds = outputs.detach().argmax(dim=1)         # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã (–∏–Ω–¥–µ–∫—Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ç–∞)
                correct += (preds == lbls).sum().item()        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á—ë—Ç—á–∏–∫ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
                total += lbls.size(0)                           # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤
                val_preds.extend(preds.cpu().tolist())         # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–¥–ª—è –º–µ—Ç—Ä–∏–∫)
                val_labels.extend(lbls.cpu().tolist())         # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –º–µ—Ç–∫–∏

        # –ü–æ–¥—Å—á—ë—Ç –∏—Ç–æ–≥–æ–≤—ã—Ö –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        val_loss /= total                                      # –°—Ä–µ–¥–Ω–∏–π loss –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        val_acc = correct / total                              # –î–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö
        val_f1 = f1_score(val_labels, val_preds, average='macro')  # –ú–∞–∫—Ä–æ-F1 –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏

        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤ –ª–æ–≥ –∏ TensorBoard
        logging.info(f"Val   Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}")
        writer.add_scalar("Loss/val", val_loss, epoch)
        writer.add_scalar("Acc/val", val_acc, epoch)
        writer.add_scalar("F1/val", val_f1, epoch)
        writer.add_scalar("LearningRate", optimizer.param_groups[-1]['lr'], epoch)  # –õ–æ–≥–∏—Ä—É–µ–º —Ç–µ–∫—É—â—É—é —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è

        # === –£—Å–ª–æ–≤–∏–µ —É–ª—É—á—à–µ–Ω–∏—è ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å —Ç–æ–ª—å–∫–æ –ø—Ä–∏ —Ä–æ—Å—Ç–µ F1
        if val_f1 > best_val_f1 + 1e-4:  # –ü–æ—Ä–æ–≥ 1e-4 –∑–∞—â–∏—â–∞–µ—Ç –æ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑-–∑–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö —Ñ–ª—É–∫—Ç—É–∞—Ü–∏–π
            best_val_f1 = val_f1
            epochs_no_improve = 0  # –°–±—Ä–æ—Å —Å—á—ë—Ç—á–∏–∫–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

            # –°–æ–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –º–æ–¥–µ–ª–∏, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ –ø—Ä–æ—á–µ–≥–æ
            ckpt = {
                'epoch': epoch,
                'features_state': features.state_dict(),       # –í–µ—Å–∞ feature extractor'–∞
                'optim_state': optimizer.state_dict(),         # –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
                'sched_state': scheduler.state_dict(),         # –°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞
                'scaler_state': scaler.state_dict(),           # –°–æ—Å—Ç–æ—è–Ω–∏–µ AMP scaler‚Äô–∞
                'best_val_f1': best_val_f1,                    # –ù–∞–∏–ª—É—á—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ F1
                'best_val_loss': best_val_loss                 # (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø–æ—Ç–æ–º)
            }
            if arc_head is not None:
                ckpt['arc_head_state'] = arc_head.state_dict()  # –°–æ—Ö—Ä–∞–Ω—è–µ–º ArcFace –≥–æ–ª–æ–≤—É, –µ—Å–ª–∏ –µ—Å—Ç—å

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —ç–ø–æ—Ö–∏ –∏ F1 –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞
            archive_path = os.path.join(model_dir, f"{model_name}_arcface_ep{epoch}_f1{val_f1:.4f}.pth")
            torch.save(ckpt, archive_path)
            logging.info(f"üì¶ Checkpoint saved (F1 improved): {archive_path}")

            # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª –±–µ–∑ –ø—Ä–∏–≤—è–∑–∫–∏ –∫ —ç–ø–æ—Ö–µ ‚Äî –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            simple_path = os.path.join(model_dir, f"{model_name}_arcface.pth")
            torch.save(ckpt, simple_path)
            logging.info(f"üìÑ Simple checkpoint saved: {simple_path}")

        else:
            epochs_no_improve += 1  # –ï—Å–ª–∏ F1 –Ω–µ —É–ª—É—á—à–∏–ª–æ—Å—å, —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á—ë—Ç—á–∏–∫ —Å—Ç–∞–≥–Ω–∞—Ü–∏–∏
            logging.info(f"No Val F1 improvement: {epochs_no_improve}/{patience}")

        # === Early Stopping ‚Äî –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ —É–ª—É—á—à–∞–µ—Ç—Å—è N —ç–ø–æ—Ö –ø–æ–¥—Ä—è–¥
        if epochs_no_improve >= patience:
            logging.info(f"üö© Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch}: {patience} —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏–π –ø–æ Val F1.")
            break  # –ü—Ä–µ—Ä—ã–≤–∞–µ–º —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è

    # === –ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö —ç–ø–æ—Ö –∏–ª–∏ Early Stopping ===
    if skipped_steps > 0:
        logging.warning(
            f"‚ö†Ô∏è –í —ç–ø–æ—Ö–µ {epoch} –ø—Ä–æ–ø—É—â–µ–Ω–æ {skipped_steps} scaler.step() ‚Äî –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞–ª–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (warm-up –∏–ª–∏ —Å–ª–æ–∏ –±—ã–ª–∏ –∑–∞–º–æ—Ä–æ–∂–µ–Ω—ã).")

    writer.close()  # –ó–∞–≤–µ—Ä—à–∞–µ–º –∑–∞–ø–∏—Å—å TensorBoard
    logging.info(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_name} —Å ArcFace")
